{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146d2f43",
   "metadata": {},
   "source": [
    "#  NLP (Text Mining)\n",
    "Text Mining is the process of deriving meaningful information from natural language text.\n",
    "Overall goal is to convert text into data for analysis via NLP\n",
    "Natural Language Processing(NLP) is a part of computer science and artificial intelligence which deals with human languages.\n",
    "NLP is a component of text mining that performs a special kind of linguistic analysis that essentially helps a machine “read” text. It uses a different methodology to decipher the ambiguities in human language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814940e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Mining Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a16363e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import nltk.corpus #  \n",
    "from nltk.tokenize import word_tokenize # tokenization\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist   # Frequency of words\n",
    "from nltk.stem import PorterStemmer   # Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d74dc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\du\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt')  #download this NLP library once and then comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dc8ef",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5dca742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Brazil they drive on the right-hand side of the road. Brazil has a  large coastline on the eastern side of South America\n"
     ]
    }
   ],
   "source": [
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a  \" + \"large coastline on the eastern side of South America\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a712ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'side',\n",
       " 'of',\n",
       " 'South',\n",
       " 'America']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96af461c",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "839ea8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Brazil',\n",
       " 'they',\n",
       " 'drive',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right-hand',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " '.',\n",
       " 'Brazil',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'coastline',\n",
       " 'on',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'side',\n",
       " 'of',\n",
       " 'South',\n",
       " 'America']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(text)\n",
    "token\n",
    "#for i in token : print(i, sep ='\\t ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab4c00",
   "metadata": {},
   "source": [
    "text split into tokens. Words, comma, punctuations are called tokens.\n",
    "### Finding frequency distinct in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c86401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "fdist = FreqDist(token)\n",
    "fdist\n",
    "#‘the’ is found 3 times in the text, ‘Brazil’ is found 2 times in the text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcf12671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('Brazil', 2),\n",
       " ('on', 2),\n",
       " ('side', 2),\n",
       " ('of', 2),\n",
       " ('In', 1),\n",
       " ('they', 1),\n",
       " ('drive', 1),\n",
       " ('right-hand', 1),\n",
       " ('road', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb2787",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming usually refers to normalizing words into its base form or root form\n",
    "Here, we have words waited, waiting and waits. Here the root word is ‘wait’. \n",
    "-There are two methods in Stemming namely, Porter Stemming (removes common morphological and inflectional endings from words) and Lancaster Stemming (a more aggressive stemming algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfc5c7",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa89c0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00f4dcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words; actual word and its stem/root\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word + \":\" + pst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be9a22",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer\n",
    "Lancaster is more aggressive than Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec593b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    " print(word+ ':' + lst.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30e981",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "it is the process of converting a word to its base form. The difference between stemming and \n",
    "lemmatization is, lemmatization considers the context and converts the word to its meaningful\n",
    "base form, whereas stemming just removes the last few characters, often leading to incorrect\n",
    "meanings and spelling errors. For example, lemmatization would correctly identify the base\n",
    "form of ‘caring’ to ‘care’, \n",
    "whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "Lemmatization can be implemented in python by using Wordnet Lemmatizer, \n",
    "Spacy Lemmatizer, TextBlob, Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30a4e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Lemmatizer library from nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('omw-1.4')  #download this once and then comment\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de1c2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n"
     ]
    }
   ],
   "source": [
    "print('rocks :', lemmatizer.lemmatize('rocks')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38435aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "print('corpora :', lemmatizer.lemmatize('corpora'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9333e13",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, \n",
    "“above”, “on”, “is”, “all”. These words do not provide any meaning and are usually \n",
    "removed from texts. We can remove these stop words using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5060a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\n"
     ]
    }
   ],
   "source": [
    "# importing stopwords from nltk library\n",
    "from nltk import word_tokenize\n",
    "#nltk.download('stopwords')  #download this library\n",
    "from nltk.corpus import stopwords\n",
    "a = set(stopwords.words('english'))\n",
    "text = \"Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21824df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
     ]
    }
   ],
   "source": [
    "text1 = word_tokenize(text.lower())  #all to lowercase\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eff7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
     ]
    }
   ],
   "source": [
    "stopwords = [x for x in text1 if x not in a]\n",
    "print(stopwords)  # removed stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff3dab",
   "metadata": {},
   "source": [
    "## Part of speech tagging (POS)\n",
    "Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19028d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vote', 'to', 'choose', 'a', 'particular', 'man', 'or', 'a', 'group', '(', 'party', ')', 'to', 'represent', 'them', 'in', 'parliament']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\du\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')  #download this once\n",
    "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45b0d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81eed8",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1685bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svgling in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: svgwrite in c:\\programdata\\anaconda3\\lib\\site-packages (from svgling) (1.4.3)\n",
      "Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\n"
     ]
    }
   ],
   "source": [
    "text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
    "#importing chunk library from nltk\n",
    "#nltk.download('maxent_ne_chunker')  #download this once\n",
    "#nltk.download('words')   # download this once\n",
    "from nltk import ne_chunk# tokenize and POS Tagging before doing chunk\n",
    "#!pip install svgling  #install this library\n",
    "print(text)\n",
    "token = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c6f231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', '’', 's', 'CEO', 'Sundar', 'Pichai', 'introduced', 'the', 'new', 'Pixel', 'at', 'Minnesota', 'Roi', 'Centre', 'Event']\n"
     ]
    }
   ],
   "source": [
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57149918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP'), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), ('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP'), ('Event', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(token)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e660897f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,824.0,168.0\" width=\"824px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"7.76699%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Google</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.8835%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.85437%\" x=\"7.76699%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">’</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.1942%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.85437%\" x=\"12.6214%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">s</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.0485%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.3883%\" x=\"17.4757%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"23.8095%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CEO</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.9048%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"23.8095%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Sundar</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.8571%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"38.0952%\" x=\"61.9048%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Pichai</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.9524%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.6699%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"11.6505%\" x=\"37.8641%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">introduced</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.6893%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.85437%\" x=\"49.5146%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.9417%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.85437%\" x=\"54.3689%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">new</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.7961%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.79612%\" x=\"59.2233%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Pixel</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.6214%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.8835%\" x=\"66.0194%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.9612%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"23.301%\" x=\"69.9029%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"45.8333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Minnesota</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.9167%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.8333%\" x=\"45.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Roi</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Centre</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5534%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.79612%\" x=\"93.2039%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Event</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.6019%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07d8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b84d88",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining,\n",
    "chunking means a grouping of words or tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af6325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83920a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = 'We saw the yellow dog'\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = 'NP: {<DT>?<JJ>*<NN>}' \n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d2b87",
   "metadata": {},
   "source": [
    "summarizes text preprocessing and covers the NLTK steps including Tokenization, Stemming, Lemmatization, POS tagging, Named entity recognition and Chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0b078",
   "metadata": {},
   "source": [
    "## Links\n",
    "https://www.kdnuggets.com/2020/05/text-mining-python-steps-examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed918478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
