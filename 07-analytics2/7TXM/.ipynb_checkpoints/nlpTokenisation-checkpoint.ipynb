{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00acb023",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "- breaks the raw text into words, sentences - Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb6be974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = 'The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water'\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fd4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\du\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5195f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para -  The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water  \n",
      " Sentences :  The quick brown fox jumps over the lazy little dog. \n",
      "\n",
      " Alice went up the hill and got a pale of water\n"
     ]
    }
   ],
   "source": [
    "s= nltk.sent_tokenize(p1)\n",
    "print('Para - ', p1, ' \\n Sentences : ', s[0], '\\n\\n',  s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c510d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para -  The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water  \n",
      " Sentences :  The quick brown fox jumps over the lazy little dog. \n",
      "\n",
      " Alice went up the hill and got a pale of water \n",
      "\n",
      "Words in the list  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog', '.', 'Alice', 'went', 'up', 'the', 'hill', 'and', 'got', 'a', 'pale', 'of', 'water']\n"
     ]
    }
   ],
   "source": [
    "w = nltk.word_tokenize(p1)\n",
    "print('Para - ', p1, ' \\n Sentences : ', s[0], '\\n\\n',  s[1] , '\\n')\n",
    "print('Words in the list ', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f5924",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5672c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1292de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7f0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd = nlp.create_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc21fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(sbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a4e21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(p1)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207a5d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The quick brown fox jumps over the lazy little dog.,\n",
       " Alice went up the hill and got a pale of water]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a449e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92dbe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b13ddae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, quick, brown, fox, jumps, over, the, lazy, little, dog, ., Alice, went, up, the, hill, and, got, a, pale, of, water] \t\n"
     ]
    }
   ],
   "source": [
    "print([word for word in doc], '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655380ac",
   "metadata": {},
   "source": [
    "## Keras\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4960a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef3f618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog', 'alice', 'went', 'up', 'the', 'hill', 'and', 'got', 'a', 'pale', 'of', 'water'] \t\n"
     ]
    }
   ],
   "source": [
    "kw = text_to_word_sequence(p1)\n",
    "print(kw, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c00c4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef32ffe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_on_texts() missing 1 required positional argument: 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1624\\177773744.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit_on_texts() missing 1 required positional argument: 'texts'"
     ]
    }
   ],
   "source": [
    "Tokenizer.fit_on_texts(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5320afd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x2069c1623a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7106cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be4d33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('t', 8), ('h', 4), ('e', 9), ('q', 1), ('u', 3), ('i', 4), ('c', 2), ('k', 1), ('b', 1), ('r', 3), ('o', 6), ('w', 3), ('n', 3), ('f', 2), ('x', 1), ('j', 1), ('m', 1), ('p', 3), ('s', 1), ('v', 1), ('l', 7), ('a', 6), ('z', 1), ('y', 1), ('d', 2), ('g', 2)]) \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e14836dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'t': 8, 'h': 4, 'e': 9, 'q': 1, 'u': 3, 'i': 4, 'c': 2, 'k': 1, 'b': 1, 'r': 3, 'o': 6, 'w': 3, 'n': 3, 'f': 2, 'x': 1, 'j': 1, 'm': 1, 'p': 3, 's': 1, 'v': 1, 'l': 7, 'a': 6, 'z': 1, 'y': 1, 'd': 2, 'g': 2}) \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_docs, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edd2d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 \t\n"
     ]
    }
   ],
   "source": [
    "print(t.document_count, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c85d1a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 1, 't': 2, 'l': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'u': 8, 'r': 9, 'w': 10, 'n': 11, 'p': 12, 'c': 13, 'f': 14, 'd': 15, 'g': 16, 'q': 17, 'k': 18, 'b': 19, 'x': 20, 'j': 21, 'm': 22, 's': 23, 'v': 24, 'z': 25, 'y': 26} \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index, '\\t')\n",
    "#In this dictionary, we have unique integers assigned to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "101e2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ['Machine Learning Knowledge', 'Machine Learning', 'Deep Learning', 'Artificial Intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "590b9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document count 102\n"
     ]
    }
   ],
   "source": [
    "t.fit_on_texts(text2)\n",
    "print(\"The document count\",t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de21fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words OrderedDict([('t', 8), ('h', 4), ('e', 9), ('q', 1), ('u', 3), ('i', 4), ('c', 2), ('k', 1), ('b', 1), ('r', 3), ('o', 6), ('w', 3), ('n', 3), ('f', 2), ('x', 1), ('j', 1), ('m', 1), ('p', 3), ('s', 1), ('v', 1), ('l', 7), ('a', 6), ('z', 1), ('y', 1), ('d', 2), ('g', 2), ('machine', 2), ('learning', 3), ('knowledge', 1), ('deep', 1), ('artificial', 1), ('intelligence', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(\"The count of words\",t.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc4c06",
   "metadata": {},
   "source": [
    "tit_on_texts on String\n",
    "- fit_on_texts, when applied on a string text, its attributes produce different types of results.\n",
    "texts_to_sequences\n",
    "- method helps in converting tokens of text corpus into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcbbd18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequences generated from text are :  [[18, 13, 29], [18, 13], [30, 13], [31, 32]]\n"
     ]
    }
   ],
   "source": [
    "sequences = t.texts_to_sequences(text2)\n",
    "\n",
    "print(\"The sequences generated from text are : \",sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b27b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequences generated from string  Machine Learning Knowledge  are :  [[24], [5], [14], [6], [7], [11], [1], [], [3], [1], [5], [9], [11], [7], [11], [17], [], [20], [11], [4], [10], [3], [1], [16], [17], [1]]\n"
     ]
    }
   ],
   "source": [
    "sequences = t.texts_to_sequences(text2[0])\n",
    "\n",
    "print(\"The sequences generated from string \", text2[0], \" are : \",sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_to_matrix\n",
    "#Another useful method of tokenizer class is texts_to_matrix() function for converting the document into a numpy matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0484529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "t.fit_on_texts(text2)\n",
    "encoded_docs = t.texts_to_matrix(text2, mode='binary')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc706d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
