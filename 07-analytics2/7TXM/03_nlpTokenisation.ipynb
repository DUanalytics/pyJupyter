{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00acb023",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "- breaks the raw text into words, sentences - Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6be974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = 'The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water'\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0611bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80fd4805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\du\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5195f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para -  The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water  \n",
      " Sentences :  The quick brown fox jumps over the lazy little dog. \n",
      "\n",
      " Alice went up the hill and got a pale of water\n"
     ]
    }
   ],
   "source": [
    "s= nltk.sent_tokenize(p1)\n",
    "print('Para - ', p1, ' \\n Sentences : ', s[0], '\\n\\n',  s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c510d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para -  The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water  \n",
      " Sentences :  The quick brown fox jumps over the lazy little dog. \n",
      "\n",
      " Alice went up the hill and got a pale of water \n",
      "\n",
      "Words in the list  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog', '.', 'Alice', 'went', 'up', 'the', 'hill', 'and', 'got', 'a', 'pale', 'of', 'water']\n"
     ]
    }
   ],
   "source": [
    "w = nltk.word_tokenize(p1)\n",
    "print('Para - ', p1, ' \\n Sentences : ', s[0], '\\n\\n',  s[1] , '\\n')\n",
    "print('Words in the list ', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f5924",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5672c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1292de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c537761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Defaults', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_format_docs_and_golds', '_meta', '_multiprocessing_pipe', '_optimizer', '_path', 'add_pipe', 'begin_training', 'create_pipe', 'disable_pipes', 'entity', 'evaluate', 'factories', 'from_bytes', 'from_disk', 'get_pipe', 'has_pipe', 'lang', 'linker', 'make_doc', 'matcher', 'max_length', 'meta', 'parser', 'path', 'pipe', 'pipe_factories', 'pipe_labels', 'pipe_names', 'pipeline', 'preprocess_gold', 'rehearse', 'remove_pipe', 'rename_pipe', 'replace_pipe', 'resume_training', 'tagger', 'tensorizer', 'to_bytes', 'to_disk', 'tokenizer', 'update', 'use_params', 'vocab'] \t\n"
     ]
    }
   ],
   "source": [
    "print(dir(nlp), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7f0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd = nlp.create_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc21fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(sbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4e21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The quick brown fox jumps over the lazy little dog. Alice went up the hill and got a pale of water"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(p1)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207a5d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The quick brown fox jumps over the lazy little dog.,\n",
       " Alice went up the hill and got a pale of water]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a449e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92dbe3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13ddae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, quick, brown, fox, jumps, over, the, lazy, little, dog, ., Alice, went, up, the, hill, and, got, a, pale, of, water] \t\n"
     ]
    }
   ],
   "source": [
    "print([word for word in doc], '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655380ac",
   "metadata": {},
   "source": [
    "## Keras\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4960a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3f618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog', 'alice', 'went', 'up', 'the', 'hill', 'and', 'got', 'a', 'pale', 'of', 'water'] \t\n"
     ]
    }
   ],
   "source": [
    "kw = text_to_word_sequence(p1)\n",
    "print(kw, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00c4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5320afd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x182cd223a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7106cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4d33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('t', 8), ('h', 4), ('e', 9), ('q', 1), ('u', 3), ('i', 4), ('c', 2), ('k', 1), ('b', 1), ('r', 3), ('o', 6), ('w', 3), ('n', 3), ('f', 2), ('x', 1), ('j', 1), ('m', 1), ('p', 3), ('s', 1), ('v', 1), ('l', 7), ('a', 6), ('z', 1), ('y', 1), ('d', 2), ('g', 2)]) \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_counts, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2206ee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', 6), ('b', 1), ('c', 2), ('d', 2), ('e', 9), ('f', 2), ('g', 2), ('h', 4), ('i', 4), ('j', 1), ('k', 1), ('l', 7), ('m', 1), ('n', 3), ('o', 6), ('p', 3), ('q', 1), ('r', 3), ('s', 1), ('t', 8), ('u', 3), ('v', 1), ('w', 3), ('x', 1), ('y', 1), ('z', 1)]) \t\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "print(OrderedDict(sorted(t.word_counts.items())), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84cfc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 1, 'k': 1, 'b': 1, 'x': 1, 'j': 1, 'm': 1, 's': 1, 'v': 1, 'z': 1, 'y': 1, 'c': 2, 'f': 2, 'd': 2, 'g': 2, 'u': 3, 'r': 3, 'w': 3, 'n': 3, 'p': 3, 'h': 4, 'i': 4, 'o': 6, 'a': 6, 'l': 7, 't': 8, 'e': 9} \t\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted(t.word_counts.items(), key=lambda item: item[1])), '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e854d6f",
   "metadata": {},
   "source": [
    "An OrderedDict is a dictionary subclass that remembers the order that keys were first inserted. The only difference between dict() and OrderedDict() is that:\n",
    "\n",
    "OrderedDict preserves the order in which the keys are inserted. A regular dict doesn’t track the insertion order and iterating it gives the values in an arbitrary order. By contrast, the order the items are inserted is remembered by OrderedDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e14836dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'t': 8, 'h': 4, 'e': 9, 'q': 1, 'u': 3, 'i': 4, 'c': 2, 'k': 1, 'b': 1, 'r': 3, 'o': 6, 'w': 3, 'n': 3, 'f': 2, 'x': 1, 'j': 1, 'm': 1, 'p': 3, 's': 1, 'v': 1, 'l': 7, 'a': 6, 'z': 1, 'y': 1, 'd': 2, 'g': 2}) \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_docs, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c64d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edd2d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 \t\n"
     ]
    }
   ],
   "source": [
    "print(t.document_count, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c85d1a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 1, 't': 2, 'l': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'u': 8, 'r': 9, 'w': 10, 'n': 11, 'p': 12, 'c': 13, 'f': 14, 'd': 15, 'g': 16, 'q': 17, 'k': 18, 'b': 19, 'x': 20, 'j': 21, 'm': 22, 's': 23, 'v': 24, 'z': 25, 'y': 26} \t\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index, '\\t')\n",
    "#In this dictionary, we have unique integers assigned to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd0f168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 5, 'b': 19, 'c': 13, 'd': 15, 'e': 1, 'f': 14, 'g': 16, 'h': 6, 'i': 7, 'j': 21, 'k': 18, 'l': 3, 'm': 22, 'n': 11, 'o': 4, 'p': 12, 'q': 17, 'r': 9, 's': 23, 't': 2, 'u': 8, 'v': 24, 'w': 10, 'x': 20, 'y': 26, 'z': 25} \t\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted(t.word_index.items())), '\\t')   # integers not assigned in alpha order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bb31617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 1, 't': 2, 'l': 3, 'o': 4, 'a': 5, 'h': 6, 'i': 7, 'u': 8, 'r': 9, 'w': 10, 'n': 11, 'p': 12, 'c': 13, 'f': 14, 'd': 15, 'g': 16, 'q': 17, 'k': 18, 'b': 19, 'x': 20, 'j': 21, 'm': 22, 's': 23, 'v': 24, 'z': 25, 'y': 26} \t\n"
     ]
    }
   ],
   "source": [
    "print(dict(sorted(t.word_index.items(), key=lambda item: item[1])), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "101e2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ['Machine Learning Knowledge', 'Machine Learning', 'Deep Learning', 'Artificial Intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "590b9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document count 106\n"
     ]
    }
   ],
   "source": [
    "t.fit_on_texts(text2)\n",
    "print(\"The document count\",t.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de21fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words OrderedDict([('t', 8), ('h', 4), ('e', 9), ('q', 1), ('u', 3), ('i', 4), ('c', 2), ('k', 1), ('b', 1), ('r', 3), ('o', 6), ('w', 3), ('n', 3), ('f', 2), ('x', 1), ('j', 1), ('m', 1), ('p', 3), ('s', 1), ('v', 1), ('l', 7), ('a', 6), ('z', 1), ('y', 1), ('d', 2), ('g', 2), ('machine', 4), ('learning', 6), ('knowledge', 2), ('deep', 2), ('artificial', 2), ('intelligence', 2)])\n"
     ]
    }
   ],
   "source": [
    "print(\"The count of words\",t.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc4c06",
   "metadata": {},
   "source": [
    "fit_on_texts on String\n",
    "- fit_on_texts, when applied on a string text, its attributes produce different types of results.\n",
    "texts_to_sequences\n",
    "- method helps in converting tokens of text corpus into a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dcbbd18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequences generated from text are :  [[18, 13, 29], [18, 13], [30, 13], [31, 32]]\n"
     ]
    }
   ],
   "source": [
    "sequences = t.texts_to_sequences(text2)\n",
    "\n",
    "print(\"The sequences generated from text are : \",sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b27b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sequences generated from string  Machine Learning Knowledge  are :  [[24], [5], [14], [6], [7], [11], [1], [], [3], [1], [5], [9], [11], [7], [11], [17], [], [20], [11], [4], [10], [3], [1], [16], [17], [1]]\n"
     ]
    }
   ],
   "source": [
    "sequences = t.texts_to_sequences(text2[0])\n",
    "\n",
    "print(\"The sequences generated from string \", text2[0], \" are : \",sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_to_matrix\n",
    "#Another useful method of tokenizer class is texts_to_matrix() function for converting the document into a numpy matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0484529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "t.fit_on_texts(text2)\n",
    "encoded_docs = t.texts_to_matrix(text2, mode='binary')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc706d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
