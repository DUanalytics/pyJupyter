{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad46375e",
   "metadata": {},
   "source": [
    "## Bag of words  TF-IDF\n",
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears\n",
    "https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ee4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 code for preprocessing text\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121e2b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy little dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the text here as :\n",
    "text1 = \"The quick brown fox jumps over the lazy little dog\"\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f831148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy little dog']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = nltk.sent_tokenize(text1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6ee4a",
   "metadata": {},
   "source": [
    "- Convert text to lower case.\n",
    "- Remove all non-word characters.\n",
    "- Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e99da289",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(text1)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c8f840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick brown fox jumps over the lazy little dog']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b61a0d",
   "metadata": {},
   "source": [
    "Obtaining most frequent words in our text. We will apply the following steps to generate our model.\n",
    "- We declare a dictionary to hold our bag of words.\n",
    "- Next we tokenize each sentence to words.\n",
    "- Now for each word in sentence, we check if the word exists in our dictionary.\n",
    "- If it does, then we increment its count by 1. If it doesnâ€™t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2831bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5320db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab81bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdbd221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'little', 'dog']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2b516",
   "metadata": {},
   "source": [
    "### step 3 : Building the Bag of Words model\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.\n",
    "This can be implemented with the help of following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f197c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759a7dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a0927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a912a418",
   "metadata": {},
   "source": [
    "### TFid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110985da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
    "sentence_2=\"This is not good at all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89327f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Smoothing:\n",
      "       good       job      miss\n",
      "0  0.385372  0.652491  0.652491\n",
      "1  1.000000  0.000000  0.000000\n",
      "\n",
      "\n",
      "With Smoothing:\n",
      "       good       job      miss\n",
      "0  0.449436  0.631667  0.631667\n",
      "1  1.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "#without smooth IDF\n",
    "print(\"Without Smoothing:\")\n",
    "#define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True,  smooth_idf=False,  ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names_out())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")\n",
    " \n",
    "#with smooth\n",
    "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  smooth_idf=True,  ngram_range=(1,1),stop_words='english')\n",
    " \n",
    " \n",
    "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "print(\"With Smoothing:\")\n",
    "tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names_out())\n",
    "print(tf_idf_dataframe_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffb2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
