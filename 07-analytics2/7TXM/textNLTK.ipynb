{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfd5ab5",
   "metadata": {},
   "source": [
    "# Analyse Text in Python using NLTK\n",
    "- it usese common algorithms such as tokenisation, part of speech tagging, stemming, sentiment analysis, topic segmentations, named entitty recognition\n",
    "- !pip install nltk\n",
    "- https://machinelearninggeek.com/text-analytics-for-beginners-using-python-nltk/\n",
    "- https://pypi.org/project/gensim/ : topic modelling, document indexing and similarity retrieval with large corpora\n",
    "- https://pypi.org/project/Pattern/  Web mining module for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9380cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install gensim #semantic modelling\n",
    "#!pip install pattern # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc345a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pydataset import data\n",
    "np.set_printoptions(precision=2)\n",
    "pd.set_option(\"display.precision\", 1)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import nltk\n",
    "#nltk.download()# download data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13cd5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#short eg\n",
    "s1 = 'The quick brown fox jumps over the lazy little dog'\n",
    "from nltk.tokenize import word_tokenize\n",
    "s1_tokens = nltk.word_tokenize(s1)\n",
    "print (s1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfb38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73d08d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog']\n",
      "After Stemming  ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'littl', 'dog']\n"
     ]
    }
   ],
   "source": [
    "sw = []\n",
    "for w in s1_tokens:\n",
    "    sw.append(ps.stem(w))\n",
    "print('Before Stemming ', s1_tokens)\n",
    "print('After Stemming ', sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23199932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1bd231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'little', 'dog']\n",
      "PoS tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('little', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "sent = s1 # \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "tokens=word_tokenize(sent)\n",
    "pos_=pos_tag(tokens)\n",
    "print(\"Tokens:\",tokens)\n",
    "print(\"PoS tags:\",pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1d386a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('little', 'JJ'), ('dog', 'NN')] \n",
      "\n",
      "(S\n",
      "  The/DT\n",
      "  quick/JJ\n",
      "  brown/NN\n",
      "  fox/NN\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  lazy/JJ\n",
      "  little/JJ\n",
      "  dog/NN)\n"
     ]
    }
   ],
   "source": [
    "#from nltk import ne_chunk\n",
    "import nltk\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(tagged ,'\\n')\n",
    "print(entities, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1b3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()\n",
    "#https://www.nltk.org/howto/corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4103f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.tagged_sents()[0]\n",
    "#[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
    "#treebank_chunk.chunked_sents()[0]\n",
    "#Tree('S', [Tree('NP', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('NP', [('61', 'CD'), ('years', 'NNS')]), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), Tree('NP', [('the', 'DT'), ('board', 'NN')]), ('as', 'IN'), Tree('NP', [('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])\n",
    "#treebank_chunk.chunked_sents()[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detailed Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello Mr Students, Welcome to this training on Data Science with Python. I hope you are enjoying the classes. Please share your feedback at the end of the classes. This class is happening from Noida, India\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefffcae",
   "metadata": {},
   "source": [
    "Basic feature extraction using text data\n",
    "- Number of words, Number of characters , Average word length, Number of stopwords, Number of special characters, Number of numerics ,Number of uppercase words\n",
    "Basic Text Pre-processing of text data\n",
    "- Lower casing, Punctuation removal ,Stopwords removal, Frequent words removal, Rare words removal, Spelling correction, Tokenization, Stemming, Lemmatization\n",
    "Advance Text Processing\n",
    "- N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Word, Sentiment Analysis , Word Embedding\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "https://kfsyscc.github.io/attachments/IT/Text_Analytics_with_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b5567",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- breaking text paragraphs into smaller chunks - words or sentences\n",
    "- nltk.tokenize \n",
    "    - sent_tokenize - text into sentences\n",
    "    - word_tokenize - text into words\n",
    "    - wordPuncttokenizer - text into words and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4512c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('S1 \\t', sent_tokenize(text)[0])\n",
    "print('S2 \\t', sent_tokenize(text)[1])\n",
    "print('S3 \\t', sent_tokenize(text)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word\n",
    "wordTokens = word_tokenize(text)\n",
    "print(wordTokens)\n",
    "# para to different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of words\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(wordTokens)\n",
    "print(fdist)\n",
    "#fdist.  # see the functions avl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(28,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some words don't make any sense . the\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81683991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove such words from the text\n",
    "filtered_tokens = []\n",
    "for w in wordTokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Tokenized Words (Original) \\t',  wordTokens)\n",
    "print('\\nFiltered Words (Removing Stop Words) \\t', filtered_tokens)\n",
    "# punctuations still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f046a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "print(punctuations)\n",
    "#for i in punctuations:    print(i, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "filtered_tokens2 = []\n",
    "for i in filtered_tokens:\n",
    "    if i not in punctuations:\n",
    "        #print(i)\n",
    "        filtered_tokens2.append(i)\n",
    "print(' After Removing Punctuations \\t', filtered_tokens2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f2ca1",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Process of linguistic normalisation which reduces words to theri word root word or chops off the derivational affixes\n",
    "- eg. connection, connected, connecting  - reduct to common word connect\n",
    "- https://www.geeksforgeeks.org/introduction-to-stemming/\n",
    "- Porter’s Stemmer algorithm \n",
    "    - used to extract the base form of words\n",
    "    - nltk.stem.porter ---- PorterStemmer\n",
    "    - eg 'write' will the output of word 'Writing'\n",
    "    - It is one of the most popular stemming methods proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity. The main applications of Porter Stemmer include data mining and Information retrieval. However, its applications are only limited to English words. Also, the group of stems is mapped on to the same stem and the output stem is not necessarily a meaningful word. The algorithms are fairly lengthy in nature and are known to be the oldest stemmer.\n",
    "\n",
    "- Example: EED -> EE means “if the word has at least one vowel and consonant plus EED ending, change the ending to EE” as ‘agreed’ becomes ‘agree’. \n",
    "\n",
    "    - Lovins Stemmer \n",
    "    - Dawson Stemmer \n",
    "    - Krovetz Stemmer\n",
    "    - Xerox Stemmer\n",
    "    - N-Gram Stemmer \n",
    "    - Snowball Stemmer\n",
    "    - Lancaster Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec240f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = []\n",
    "for w in filtered_tokens2:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "print('Before Stemming ', filtered_tokens2)\n",
    "print('After Stemming ', stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725a209",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "- Another way to extract the base form of words, normally aiming to remove inflectional endings by using vocabulary ad morphological analysis. \n",
    "    -reduces words to their bases word\n",
    "- The word “better” has “good” as its lemma.\n",
    "- packages : nltk.stem ----\n",
    "    - WordNetLemmatizer\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f903df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexicon Normalization#performing stemming and Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "word = \"flying\"\n",
    "\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc155a",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "-  Identification of parts of speech(POS) and short phrases can be done with the help of chunking.\n",
    "- Tokenisation is for breaking into smaller parts; chunking labels these tokens. We can get the structure of sentence with the help of chunking process\n",
    "- Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "- Steps\n",
    "    - Chunk grammar definition - define the grammar for chunking. Consists of rules which we need to follow\n",
    "        - grammar = 'NP:{<DT>?<JJ>*<NN>}'\n",
    "    - Chunk parser creation - create a chunk parser which would parser the grammar and give output\n",
    "        - parser_chunking = nltk.RegexParser(grammar)\n",
    "        - parser_chunking.parse(sentence)\n",
    "    - Output - output in tree format\n",
    "        - Output = parser_chunking.parse(sentence)\n",
    "        - output.draw()\n",
    "- Some of the short codes are\n",
    "    - DT  - is the determinant\n",
    "    - VBP - is the verb\n",
    "    - JJ - is adjective\n",
    "    - IN - is the proposition\n",
    "    - NN - is the noun\n",
    "- Displayed as pair\n",
    "    - ('a', 'DT), ('clever', 'JJ') , ('fox' , 'NN'), (was, VBP), (jumping, VBP), (over, IN), (the, DT), (wall, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8abebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "\n",
    "tokens=word_tokenize(sent)\n",
    "pos_=pos_tag(tokens)\n",
    "\n",
    "\n",
    "print(\"Tokens:\",tokens)\n",
    "print(\"PoS tags:\",pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PoS tags:\",pos_tag(filtered_tokens2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36b732",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "- Named entity recognition is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. We can detect entities using ne_chunk() function available in NLTK. Let’s see the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "sent=\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\"\n",
    "\n",
    "for chunk in ne_chunk(nltk.pos_tag(word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d13a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in ne_chunk(nltk.pos_tag(filtered_tokens2)):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3bdd2b",
   "metadata": {},
   "source": [
    "## NLTK  \n",
    "- The NLTK corpus is a massive dump of all kinds of natural language data sets that are definitely worth taking a look at. Almost all of the files in the NLTK corpus follow the same rules for accessing them by using the NLTK module, but nothing is magical about them.\n",
    "- A corpus can be assembled from a variety of sources and genres. Such a corpus can be used for general NLP tasks. On the other hand, a corpus might be from a single source, domain or genre. Such a corpus can be used only for a specific purpose\n",
    "- https://www.nltk.org/api/nltk.corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5e748",
   "metadata": {},
   "source": [
    "he Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
    "https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d669be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(brown.words())) \n",
    "#list of the words in the Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(brown.categories()), brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentences\n",
    "sentences1 = brown.sents(categories = 'mystery')\n",
    "print(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1B = [' '.join(sentence_token) for sentence_token in sentences1] \n",
    "sentences1B[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f47b0",
   "metadata": {},
   "source": [
    "### Reuter\n",
    "The Reuters Corpus consists of 10,788 Reuters news documents from around 90 different\n",
    "categories and has been grouped into train and test sets. In machine learning terminology,\n",
    "train sets are usually used to train a model, and test sets are used to test the performance of\n",
    "that model. The following code snippet shows how to access the data for the Reuters Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reuters.categories()), reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = reuters.sents(categories=['housing', 'income'])\n",
    "sentences2B = [' '.join(sentence_tokens) for sentence_tokens in sentences2]\n",
    "print(sentences2B[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a52968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences2B[-1].split(), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe949d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get contents from webpage\n",
    "import bs4\n",
    "import urllib.request\n",
    "link='https://www.g20.org/en/media-resources/speeches/march-23/fmm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage=str(urllib.request.urlopen(link).read())\n",
    "soup = bs4.BeautifulSoup(webpage)\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa53949",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmspeech1 = soup.get_text()\n",
    "type(pmspeech1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef6f51",
   "metadata": {},
   "source": [
    "newStr =[]\n",
    "for str in pmspeech1:\n",
    "    newStr.append(str.replace('\\n', ''))\n",
    "print(newStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a668d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms1 = pmspeech1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eca564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeList = ['\\\\n', '\\\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms2 = pms1.replace('\\\\n','')\n",
    "pms2 = pms2.replace('\\\\t','')\n",
    "pms2 = pms2.replace('\\\\','')\n",
    "print(pms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pms2 = re.sub(' +', ' ', pms2)  #note space in ' +'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7827e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'(\\s)#\\w+', r'\\1', 'Hello all please help #me2   but#notme')\n",
    "# [\\w’]+ signals that the code should find all the alphanumeric characters \n",
    "# until any other character is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e911e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english')) # these types of words will be remove. \n",
    "# we can create our custom list also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a26a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myWordList = ['Reserved','Modi', 'brown']\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.extend(myWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_stopwords, '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text), '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b63234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove stop words from existing dictionary\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "#https://stackabuse.com/removing-stop-words-from-strings-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords.remove('Modi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_stopwords, '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b6179",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split words from text\n",
    "pms2.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a39954",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(pms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(pms2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e2413",
   "metadata": {},
   "source": [
    "#from spacy.lang.en import English\n",
    "#!pip install keras\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "#text_to_word_sequence(text)\n",
    "#from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "text = ' '.join([word for word in text.split() if word not in stopwords_dict])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc66078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pms3 = ' '.join([word for word in pms2.split() if word not in stopwords_dict])\n",
    "print(pms3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6351d9",
   "metadata": {},
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "text = pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344d467",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
